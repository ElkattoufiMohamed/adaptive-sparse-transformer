# Base configuration for all experiments
model:
  vocab_size: 30522 # BERT tokenizer vocab size
  dim: 768
  depth: 12
  num_heads: 12
  mlp_ratio: 4.0
  max_seq_len: 512
  dropout: 0.1
  num_classes: 2

# Adaptive attention specific parameters
attention:
  local_window_size: 64
  global_ratio: 0.1
  learnable_sparsity: true
  temperature: 1.0

# Training configuration
training:
  batch_size: 16
  learning_rate: 2e-5
  num_epochs: 5
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100

# Data configuration
data:
  dataset_name: "imdb"
  max_length: 512
  train_split: "train"
  eval_split: "test"
  num_workers: 4

# Experiment tracking
wandb:
  project: "adaptive-sparse-transformer"
  entity: "mohamedelkattoufi"
  tags: ["transformer", "attention", "sparse"]
