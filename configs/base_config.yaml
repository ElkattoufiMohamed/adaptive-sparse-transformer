# Base configuration for all experiments
model:
  vocab_size: 30522 # BERT tokenizer vocab size
  dim: 768
  depth: 6
  num_heads: 12
  mlp_ratio: 4.0
  max_seq_len: 512
  dropout: 0.1
  num_classes: 2

# Adaptive attention specific parameters
attention:
  local_window_size: 64
  global_ratio: 0.1
  learnable_sparsity: true
  temperature: 1.0

# Training configuration
training:
  batch_size: 4
  learning_rate: 0.0001
  num_epochs: 5
  warmup_steps: 0
  lr_scheduler: "constant"
  weight_decay: 0.01
  gradient_clip_norm: false #1.0
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  seed: 42

# Data configuration
data:
  dataset_name: "imdb"
  max_length: 256
  train_split: "train"
  eval_split: "test"
  num_workers: 4

# Experiment tracking
wandb:
  project: "adaptive-sparse-transformer"
  entity: "mohamedelkattoufi-fsm"
  tags: ["transformer", "attention", "sparse"]

# Evaluation Configuration
evaluation:
  eval_steps: 500
  eval_batch_size: 4
  save_best_model: true
  early_stopping_patience: 10
  metrics_to_track: ["loss", "accuracy"]
  save_steps: 1000
  logging_steps: 100
  max_eval_samples: null
  metric_for_best_model: "loss"
  greater_is_better: false

# Hardware Configuration
hardware:
  mixed_precision: false
  device: "auto" # or "cuda", "cpu"
  num_workers: 4
  pin_memory: false

scheduler:
  type: "none"
