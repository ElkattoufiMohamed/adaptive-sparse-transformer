model:
  vocab_size: 30522
  dim: 768
  depth: 4 #8
  num_heads: 12
  mlp_ratio: 4.0
  max_seq_len: 512
  dropout: 0.1
  num_classes: 2

attention:
  local_window_size: 64
  global_ratio: 0.1
  learnable_sparsity: true
  temperature: 1.0
  pattern_temperature: 1.0
  min_pattern_temperature: 0.3
  pattern_dropout: 0.1

training:
  batch_size: 16
  learning_rate: 0.0002
  pattern_lr_multiplier: 10.0
  num_epochs: 30
  warmup_steps: 1000
  lr_scheduler: "cosine"
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  save_steps: 1000
  eval_steps: 500
  logging_steps: 50
  seed: 42

data:
  dataset_name: "cola"
  max_length: 256
  train_split: "train"
  eval_split: "test"
  num_workers: 4

wandb:
  project: "adaptive-sparse-transformer"
  entity: "mohamedelkattoufi-fsm"
  tags: ["transformer", "attention", "sparse", "pattern-learning"]
  name: "adaptive_pattern_fix_v2" # Track this specific fix

evaluation:
  eval_steps: 500
  eval_batch_size: 16
  save_best_model: true
  early_stopping_patience: 10
  metrics_to_track: ["loss", "accuracy", "pattern_entropy", "pattern_diversity"]
  save_steps: 1000
  logging_steps: 50
  max_eval_samples: 1000
  metric_for_best_model: "accuracy"
  greater_is_better: true

hardware:
  mixed_precision: true
  device: "auto"
  num_workers: 4
  pin_memory: true

scheduler:
  type: "cosine"
