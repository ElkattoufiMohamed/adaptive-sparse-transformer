model:
  vocab_size: 30522
  dim: 768
  depth: 4 #8
  num_heads: 12
  mlp_ratio: 4.0
  max_seq_len: 256
  dropout: 0.1
  num_classes: 2

attention:
  local_window_size: 64
  global_ratio: 0.1
  learnable_sparsity: true
  temperature: 1.0
  pattern_temperature: 2.0
  min_pattern_temperature: 0.5
  pattern_dropout: 0.1

training:
  batch_size: 16
  learning_rate: 0.0005
  pattern_lr_multiplier: 20.0
  num_epochs: 50
  warmup_steps: 2000
  lr_scheduler: "cosine"
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  save_steps: 1000
  eval_steps: 500
  logging_steps: 50
  seed: 42

data:
  dataset_name: ["imdb", "ag_news", "yelp_polarity", "dbpedia_14", "sst2", "trec", "amazon_polarity", "emotion"]
  tokenizer_name: bert-base-uncased
  num_workers: 4
  debug_subset_size: 4096

wandb:
  project: "adaptive-sparse-transformer"
  entity: "mohamedelkattoufi-fsm"
  tags: ["transformer", "attention", "sparse", "pattern-learning"]

evaluation:
  eval_steps: 500
  eval_batch_size: 16
  save_best_model: true
  early_stopping_patience: 10
  metrics_to_track: ["loss", "accuracy", "pattern_entropy", "pattern_diversity"]
  save_steps: 1000
  logging_steps: 50
  max_eval_samples: 1000
  metric_for_best_model: "accuracy"
  greater_is_better: true

hardware:
  mixed_precision: true
  device: "auto"
  num_workers: 4
  pin_memory: true

scheduler:
  type: "cosine_with_restarts"
  min_lr: 1e-5 
